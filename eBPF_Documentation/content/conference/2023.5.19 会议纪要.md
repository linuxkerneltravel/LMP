# 2023.5.19 会议纪要

## 内存组

**徐东 & 员可盈**

仍在之前的方向上探索，也就是研究移植BCC的memleak工具到libbpf上以完成知道现有ebpf工具对于解决内存泄露的，但是由于操作系统大赛时间紧，所以内存泄漏这块推进较少

**乔哲**

仍在忙于论文，对于此推进较少

## 网络组

**付紫阳**

**2023.05.15~19**

本周工作：

- 会议决议每人负责一个底层方向，目前我的方向为TCP重传与丢包
- 查找TCP重传机制的相关资料

下一步计划：

- 分析TCP重传的三种机制及在Linux中的实现
- 使用传统工具以及现有的eBPF工具，观测性能参数
- 分析`/proc/sys/net/ipv4/`下面涉及TCP的参数含义，以及了解如何通过参数配置进行网络调优（特化）

### 1. TCP重传机制原理简述

关于TCP重传机制的阐述，网络上有大量的优质文章，读者可以搜索查阅。为了文章的完整性，本文仅做概述。

TCP重传机制具有以下几种方法：

- 基于计时器的重传（超时重传）

- 快重传（Fast Retransmit）
- 选择重传
  - 选择性确认方法（Selective Acknowledgment，SACK）
  - Duplicate SACK

#### 1.1 超时重传

关于超时重传，即数据发送时计时，若指定超时重传时间（Retransmission Timeout，RTO）范围内，没有收到ACK确认应答报文，将触发超时重传机制，重新发送该数据包。

- RTO不固定，根据网络延迟，动态调整
- 调整与RTT（Round Trip Time）往返时间有关

#### 1.2 快重传

快重传是TCP协议在特定情况下的一种优化重传机制，当TCP发送方连续收到三个相同的确认，就会认为这个报文是丢失了而不是延迟，此时会立即进行重传，从而减少等待超时时间的长短，提高重传的速度和效率。

#### 1.3 选择重传

SACK是TCP协议引入的一种优化重传的机制，它可以在数据包中带上一个SACK选项，记录已经成功接收的数据及缺失的数据段的起止位置。这样当TCP重传时，可以精确指定要重传的数据包，从而避免重传已经成功接收的数据包，减少网络拥塞和提高传输效率。

D-SACK（Duplicate SACK）是SACK的一个改进，它可以记录重复的SACK块，即探测到的丢失数据块的段在后续的确认中又会出现。D-SACK在与SACK一起使用时，主要解决某些复杂情况下SACK的不足，例如在网络重传造成新段落结构时，SACK存在可能会指示不正确的段。

### 2. Linux内核网络协议栈重传机制实现

#### 2.1 TCP关键参数

SYN重传次数`tcp_syn_retries`

SYN-ACK重传次数`tcp_synack_retries`

SYN包队列`tcp_max_syn_backlog`

丢包率

超时重传时间

重传队列

#### 2.2 TCP重传机制的实现

通过跟踪数据包重传的路线，理解Linux网络协议栈的具体实现。

> Linux内核版本：5.10.90

##### 2.2.1 超时重传

###### 2.2.1.1 RTO的范围

超时重传时间最大最小值的计算与`HZ`有关。`HZ`的由`CONFIG_HZ`设置，如下所示，目前这台X64主机的`CONFIG_HZ`为250，可以修改设置为100，300以及1000等。若用户不设置，则默认是100。

```bash
fzy@fzy-Lenovo:~$ cat /boot/config-5.15.0-72-generic | grep CONFIG_HZ
# CONFIG_HZ_PERIODIC is not set
# CONFIG_HZ_100 is not set
CONFIG_HZ_250=y
# CONFIG_HZ_300 is not set
# CONFIG_HZ_1000 is not set
CONFIG_HZ=250

```

因此， 超时重传时间的最小值为50ms，最大为30s。

```c
// include/net/tcp.h
#define TCP_RTO_MAX	((unsigned)(120*HZ))
#define TCP_RTO_MIN	((unsigned)(HZ/5))
```

###### 2.2.1.2 重传计时器

```c
/*!
创建流程图， from 内核之旅
sk->sk_prot->init --指向--> tcp_v4_init_sock(这里以tcp v4 为例)
	|--->tcp_init_sock
		   |--->tcp_init_xmit_timers
				   |----->inet_csk_init_xmit_timers
				   |	       |------timer_setup
				   |----->hrtimer_init
*/
```

```c
// net/ipv4/tcp_timer.c
static void tcp_write_timer(struct timer_list *t)
{
	struct inet_connection_sock *icsk =
			from_timer(icsk, t, icsk_retransmit_timer);
	struct sock *sk = &icsk->icsk_inet.sk;

	bh_lock_sock(sk);
	if (!sock_owned_by_user(sk)) {
		tcp_write_timer_handler(sk);
	} else {
		/* delegate our work to tcp_release_cb() */
		if (!test_and_set_bit(TCP_WRITE_TIMER_DEFERRED, &sk->sk_tsq_flags))
			sock_hold(sk);
	}
	bh_unlock_sock(sk);
	sock_put(sk);
}
```

```c
// include/net/inet_connection_sock.h
/** inet_connection_sock - INET connection oriented sock
 *
 * @icsk_accept_queue:	   FIFO of established children
 * @icsk_bind_hash:	   Bind node
 * @icsk_timeout:	   Timeout
 * @icsk_retransmit_timer: Resend (no ack)
 * @icsk_rto:		   Retransmit timeout
 * @icsk_pmtu_cookie	   Last pmtu seen by socket
 * @icsk_ca_ops		   Pluggable congestion control hook
 * @icsk_af_ops		   Operations which are AF_INET{4,6} specific
 * @icsk_ulp_ops	   Pluggable ULP control hook
 * @icsk_ulp_data	   ULP private data
 * @icsk_clean_acked	   Clean acked data hook
 * @icsk_listen_portaddr_node	hash to the portaddr listener hashtable
 * @icsk_ca_state:	   Congestion control state
 * @icsk_retransmits:	   Number of unrecovered [RTO] timeouts
 * @icsk_pending:	   Scheduled timer event
 * @icsk_backoff:	   Backoff
 * @icsk_syn_retries:      Number of allowed SYN (or equivalent) retries
 * @icsk_probes_out:	   unanswered 0 window probes
 * @icsk_ext_hdr_len:	   Network protocol overhead (IP/IPv6 options)
 * @icsk_ack:		   Delayed ACK control data
 * @icsk_mtup;		   MTU probing control data
 * @icsk_probes_tstamp:    Probe timestamp (cleared by non-zero window ack)
 * @icsk_user_timeout:	   TCP_USER_TIMEOUT value
 */
struct inet_connection_sock {
	/* inet_sock has to be the first member! */
	struct inet_sock	  icsk_inet;
	struct request_sock_queue icsk_accept_queue;
	struct inet_bind_bucket	  *icsk_bind_hash;
	unsigned long		  icsk_timeout;
 	struct timer_list	  icsk_retransmit_timer;
}
```



**张小航**

本周任务：

1.上周生成火焰图查看内核协议栈函数

2.三次握手异常

**三次握手**

三次握手其实就是双方建立资源链接的过程，三次握手四次挥手其实都是在传输控制层完成的。数据包从应用层到发出，向下一次加上了TCP报头，IP报头，以太网报头。 

为什么是三次握手，不是一次或者两次？

如果只有一次握手，双方无法确认对方的接收能力是否正常；如果只有两次握手，无法防止已失效的连接请求报文段被对方接收而产生错误。一次握手时，客户端一次发送多条连接请求，服务端一次可能接收到几百万条连接请求，服务端对每一条请求都建立服务时，资源很快就会消耗完。
**第一次握手**

客户发送第一个报文段(SYN报文段)，在这个报文段中只有SYN标志置为1。这个报文段的作用是同步序号。 请注意，这个报文段中不包括确认号，也没有定义窗口大小。只有当一个报文段中包含了确认时，定义窗口大小才有意义。这个报文段还可以包含一些选项。请注意，SYN报文段是一个控制报文段，它不携带任何数据。但是，它消耗了一个序号。当数据传送开始时，序号就应当加1。我们可以说，SYN报文段不包含真正的数据，但是我们可以想象它包含了一个虚字节。

 **SYN报文段不携带任何数据，但是它要消耗一个序号。**

<img src="https://img-blog.csdnimg.cn/9672a31a83be4b24acf6886b6550a290.png" alt="img" style="zoom: 50%;" />

**第二次握手**

服务器发送第二个报文段，即SYN + ACK报文段，其中的两个标志(SYN和ACK)置为1。这个报文段有两个目的。首先，它是另一个方向上通信的SYN报文段。服务器使用这个报文段来同步它的初始序号，以便从服务器向客户发送字节。其次，服务器还通过ACK标志来确认已收到来自客户端的SYN报文段，同时给出期望从客户端收到的下一个序号。因为这个报文段包含了确认，所以它还需要定义接收窗口大小，即rwnd (由客户端使用)。

**SYN+ACK报文段不携带数据，但要消耗一个序号。**）

<img src="https://img-blog.csdnimg.cn/1a075cb6346442bea37723f9725a8dd4.png" alt="img" style="zoom: 50%;" />

**第三次握手**

客户发送第三个报文段。这仅仅是一一个ACK报文段。它使用ACK标志和确认号:字段来确认收到了第二个报文段。请注意，这个报文段的序号和SYN报文段使用的序号一样，也就是说，这个ACK报文段不消耗任何序号。客户还必须定义服务器的窗口大小。在某些实现中，连接阶段的第三个报文段可以携带客户的第一一个数据块。在这种情况下，第三个报文段必须有一个新的序号来表示数据中的第一个字节的编号。通常，第三个报文段不携带数据，因而不消耗序号。

**ACK报文段如果不携带数据就不消耗序号。**

服务端如果收到客户端的应答，就说明Client 也同意建立连接，至此 Server ==》Client方向上的连接也就建立好了。服务端也认为连接建立成功了；如果迟迟没有收到，服务端认为客户端不希望与自己建立 Server ==》客户端方向上的连接，此时会连接失败。
服务端状态：ESTABLISHED（建立连接）/ CLOSED（连接失败）

<img src="https://img-blog.csdnimg.cn/f345ea5263314ced8fa6b654597192c6.png" alt="img" style="zoom:50%;" />

```
SYN：请求同步标志，为1的时候为有效 
ACK：应答标志，表示接受到所发的数据，1为有效 
```

## 文件组

**成员1：刘田**

复现了SandFs

复现exfuse过程中出现问题，显示缺少了libbpf.h

结合网上博客阅读estfuse论文

Extension Framework for File Systems in User space



**成员2：张晓航**

操作系统大赛：看了这篇关于FUSE文件系统细粒度访问实现的论文

FGACFS: A fine-grained access control for *nix userspace file system

https://www.sciencedirect.com/science/article/abs/pii/S0167404819301798



**成员3：廉洋洋**

了解了一下ext2文件系统中在内核中的ACL的实现

修改了一部分fusedemo代码

## CPU组

**张子恒**

1.OS大赛工作：

​	（1）完成了进程图像内核模块的编写，可以用来实现对不同业务进程资源使用情况的输出：

​	利用stress-ng模拟CPU密集型进程、内存密集型进程、IO密集型进程的指令：

```
stress-ng --vm 1 --vm-bytes 7G --cpu 1 --iomix 1 --iomix-bytes 2M
```

​	运行结果：

```
[18118.664848] task_image loaded.
[18118.684679] !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[18118.684682] Load: 3.45, 1.18, 0.59
[18118.684780] PID:42702   comm:stress-ng         cpu_ratio:95 %     mem_ratio:0  %     rwfreq:0MB/s
[18118.684784] PID:42704   comm:stress-ng         cpu_ratio:73 %     mem_ratio:74 %     rwfreq:0MB/s
[18118.684789] PID:42721   comm:stress-ng         cpu_ratio:0  %     mem_ratio:0  %     rwfreq:2MB/s
```

​	（2）完成了确定异常时间点内核模块的编写，可以用来确定最近一次出现异常的时间以及原因

2.每天都在刷算法题，感受：代码的思维及编写能力得到了很好的提升

3.《真相还原》已阅读至第3章，完成了mbr程序的编写，并成功运行



**杨宁柯：**

实现一个基于CFS的用户态调度框架的最核心部分是调度器的设计，这个代码我觉得是一个初步的东西，只能创建10个进程用来模拟这个过程，同时也写出了基于CFS兼容cpu cgroups的用户态框架模拟代码，但是因为能力有限，目前也是只能用10个或者多个进程来模拟这个过程，没办法在ubuntu底下使用CPU进行数据测验。

基于CFS的用户态调度框架示例代码模拟了一个简单的进程调度器，使用了基于CFS的调度算法，并记录了进程的优先级、时间片大小和调度周期等信息。在每个调度周期结束时，更新了进程表中每个进程的剩余CPU时间和时间片大小，并使用CFS算法进行进程调度。

因为题目要求是要兼容cpu cgroups，所以在一开始是学习了一下这个东西，明白了它是如何进行工作，在此基础上进行了用户态框架方案的梳理：

1.创建若干个cpu子系统的子cgroup，将应用分配到不同的子cgroup中，其中cgroup可以根据应用类型、应用优先级等进行划分。

2.设置每个cgroup的资源限制，限制CPU资源的使用，例如通过cpuset.cfs_period_us和cpuset.cfs_quota_us来进行资源限制。

3.对cgroup中的进程按照fair queue来调度并进行排队，保证资源公平分配。假设一个进程cgroup有Quota Q和Period P，那么该进程cgroup最多使用$Q/P*100$的CPU时间，因此cgroup中的进程被等比例排队，并分配他们的cpu时间配额。

接下来就是对代码的开发，这块比较复杂，只列举了对代码的讲解等

1.头文件和结构体定义

代码包含了头文件和进程结构体定义，进程结构体包含了进程 ID、优先级、CPU 时间等信息，同时包含了线程的 cgroup 信息。该结构体还作为红黑树节点的载体，通过这种方式实现了进程优先级的排序和调度。

2.调度器初始化

schedule_init 函数用来初始化调度器，它初始化了进程列表和红黑树，以便插入和删除进程。

3.插入和删除进程

schedule_insert 和 schedule_remove 函数分别用来将进程插入和从红黑树中删除。插入操作根据进程的优先级插入到正确的位置，而删除操作通过红黑树节点删除的方式来保证调度器的高效性。

4.选择最高优先级的进程进行调度

schedule_choose 函数选择最高优先级的进程进行调度，它通过访问红黑树的根节点来选择优先级最高的进程。

5.更新进程状态

process_update 函数用来更新进程的状态信息，包括用户态 CPU 时间和内核态 CPU 时间。通过调用系统 API 可以获取进程的 CPU 时间等信息。

6.线程函数

线程函数 thread_func 执行进程具体的任务，它通过不断地执行一个计算任务模拟进程的运行过程。在这个示例中，我们通过调用 nice 函数来改变进程的优先级。函数中还启用了线程取消能力和异步取消方式，以便后续实现进程的终止和删除。

7.创建进程

create_process 函数用来创建进程，它调用了 pthread_create 函数，初始化线程 ID 和指向线程函数的指针，创建一个新的线程。

8.分配进程到 cgroup

cgroup_assign 函数用来将进程分配到对应的 cgroup 中，它调用了 cgroup_attach 函数将进程和 cgroup 关联起来。

9.示例程序

在示例程序中，先通过 cgroup_init 函数初始化一个 CPU cgroup，然后创建 4 个进程，分配到不同的 cgroup 中。在示例中我们通过改变进程的 nice 值来改变进程的优先级，再通过 usleep 方式模拟进程的执行过程。

10.终止和清理

最后，在示例程序的末尾，我们通过调用 pthread_cancel 函数终止并删除进程。同时，我们也需要调用 cgroup_destroy 函数来销毁 Cgroup，释放相关资源。

## 鸿蒙组

**南帅波**

- 完成阶段性技术报告的编写

- 完成给定时间内的1、5、15分钟内的负载值提取，并将数据绘制成折线图。



**刘冰**

- 尝试通过vm调试内核
- kprobe优化：指令跳板
- 使用hash维护内核时间戳数据
