# 2023.7.14会议纪要

## 内存组

成员：乔哲，员可盈，徐东

**本周工作：**

**徐东：**

2023.07.14 内存泄露小组
徐东汇报自己新发现的一篇极其有价值的内存泄露工具文章：https://mp.weixin.qq.com/s/mn79HCw3T_EetqrnkSUyGQ

这篇微信文章介绍了 OpenCloudOS 如何以最小成本，高效定位内存泄露路径的问题。文章首先介绍了内存泄露的概念和分类，然后对比了传统的分析工具 gdb 和 Valgrind Memcheck 的优缺点，最后提出了基于 eBPF 的通用内存泄露（增长）分析方法，包括内存分配器行为分析和缺页异常事件分析，并分享了相应的 eBPF 分析工具 memstacks 和 pgfaultstacks 的实现原理和使用方法。

对里面提到的memstacks和pgfaultstacks工具与利用内存分配数据生成火焰图来辅助分析内存泄露很有兴趣，但是翻遍了他们的github代码仓库，没有找到这个工具的源码，下载并安装了opencloudsos但是不会用，正在与社区练习，看看能不能找到这两个工具源码

## 网络组

成员：白宇宣，张小航，付紫杨

**张小航：**

1.把tcp_connection移植到libbpf

2.三次握手中影响实时性的点

**白宇宣：**

XDP_PING

利用eBPF/XDP程序在驱动层（Native模式）回复ICMP报文，从而避免进入网络协议栈产生额外开销，并且能减少其他因素的干扰，得到更真实的延迟数据

核心实现：

```c
  icmp_type = parse_icmphdr_common(&nh, data_end, &icmphdr);
	if (eth_type == bpf_htons(ETH_P_IP) && icmp_type == ICMP_ECHO) {
		/* Swap IP source and destination */
		swap_src_dst_ipv4(iphdr);
		echo_reply = ICMP_ECHOREPLY;
	} else if (eth_type == bpf_htons(ETH_P_IPV6)
		   && icmp_type == ICMPV6_ECHO_REQUEST) {
		/* Swap IPv6 source and destination */
		swap_src_dst_ipv6(ipv6hdr);
		echo_reply = ICMPV6_ECHO_REPLY;
	} else {
		goto out;
	}
```

将icmp type更改为ICMP_ECHOREPLY，并交换源/目的ip地址（以及重新计算校验和，交换mac地址）

忽略ping的第一次请求延迟较大外，其它xdp均比走网络协议栈要快一些

已提交PR：https://github.com/linuxkerneltravel/lmp/pull/460

# CPU组

成员：张子恒，杨宁柯，王越

**杨宁柯**

本周工作：

1.上周经过老师说，分类算法使用的SVM过于老旧所以选择了一个14年的新分类算法XGBoost算法，也写出了分类算法的代码

2.目前还是在做ghost的相关工作，使用代码分析器，进行一个函数分析，将用户态的代码进行分析，然后后续进行改动

下周工作：

1.把libbpf代码改善完全。

2.结合XGboost的算法，将实验完成，上报给老师如果有什么问题的话，再进行改正。

## XGboost分类算法的简介和代码

XGBoost（eXtreme Gradient Boosting）是一种基于梯度提升树（Gradient Boosting Tree）的机器学习算法，广泛应用于分类和回归问题。

以下是对XGBoost算法的详细说明：

1. 梯度提升树：梯度提升树是一种集成学习算法，通过组合多个决策树来构建预测模型。它采用前向分步算法，每一步都试图减少当前模型的损失函数。在梯度提升树中，新的决策树被设计为拟合之前模型的残差（即预测值与真实值之间的差异），以逐步改进预测效果。
2. XGBoost的优势：相比于传统的梯度提升树算法，XGBoost具有以下几个优势：
   - 正则化：XGBoost引入了正则化项来控制模型的复杂度，防止过拟合。
   - 二阶导数近似：XGBoost在目标函数中使用了关于损失函数的二阶导数的近似，以加速模型训练过程。
   - 权重的自定义损失函数：XGBoost允许用户定义自己的目标函数，并通过调整权重来处理样本不平衡问题。
   - 缺失值处理：XGBoost能够自动学习如何处理缺失值，无需进行数据预处理。
3. 目标函数：XGBoost的目标函数包含两部分，损失函数和正则化项。损失函数用于衡量预测值与真实值之间的差异，常见的损失函数包括平方损失、对数损失等。正则化项用于控制模型复杂度，防止过拟合。XGBoost的目标是最小化目标函数的值。
4. 分裂节点的选择：在构建决策树的过程中，XGBoost采用一种贪婪算法来选择分裂节点。它通过计算每个特征的增益（Gain）来评估特征的重要性，并选择增益最大的特征作为分裂节点。
5. 正则化参数：XGBoost提供了多个正则化参数，用于控制模型的复杂度和学习速度。其中包括学习率（learning rate）、最大深度（max depth）、最小子样本权重和列抽样比例等。
6. 并行化处理：XGBoost使用多线程技术和近似算法来实现高效的并行化处理。它可以利用多核CPU进行并行计算，加快模型训练速度。

​	相关性分析：

```
plt.figure(figsize=(20,20))
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False
sns.heatmap(df.corr(),cmap="YlGnBu",annot=True)
plt.title("相关性分析图")
```

无参数模型：

```
model=xgb.XGBClassifier()
# 训练模型
model.fit(X_train,y_train)
# 预测值
y_pred = model.predict(X_test)
 
 
'''
评估指标
'''
# 求出预测和真实一样的数目
true = np.sum(y_pred == y_test )
print('预测对的结果数目为：', true)
print('预测错的的结果数目为：', y_test.shape[0]-true)
# 评估指标
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,cohen_kappa_score
print('预测数据的准确率为： {:.4}%'.format(accuracy_score(y_test,y_pred)*100))
print('预测数据的精确率为：{:.4}%'.format(
      precision_score(y_test,y_pred)*100))
print('预测数据的召回率为：{:.4}%'.format(
      recall_score(y_test,y_pred)*100))
# print("训练数据的F1值为：", f1score_train)
print('预测数据的F1值为：',
      f1_score(y_test,y_pred))
print('预测数据的Cohen’s Kappa系数为：',
      cohen_kappa_score(y_test,y_pred))
# 打印分类报告
print('预测数据的分类报告为：','
',
      classification_report(y_test,y_pred))
```

```
scorel = []
for i in range(0,300,10):
    model = xgb.XGBClassifier(n_estimators=i+1,
                                 n_jobs=--4,
                                 random_state=90).fit(X_train,y_train)
    score = model.score(X_test,y_test)
    scorel.append(score)
 
print(max(scorel),(scorel.index(max(scorel))*10)+1)  #作图反映出准确度随着估计器数量的变化，51的附近最好
plt.figure(figsize=[20,5])
plt.plot(range(1,300,10),scorel)
plt.show()
```

**张子恒**

本周工作：

1. 在代码层面学习了下tracepoint实现的原理，有学习了下tracepoint中的args结构体
2. 开源之夏
3. 阅读《真相还原》

下周工作：

1. 着手开始写和整合开源之夏的工具
2. 感觉看书不能断断续续也不能时间短，所以决定花更多的时间阅读《真相还原》

## 鸿蒙组

成员：南帅波，刘冰

**刘冰**

算法题

* kmp字符串匹配算法：变长数组
* 原始除法：二分查找

《使用调用堆栈信息进行异常检测》

方法：其基本思想是从调用栈中提取返回地址，并在两个程序执行点之间生成抽象执行路径。

训练结束后，我们可以使用散列表在线监控程序执行。在进行每个系统调用时，我们记录其虚拟栈列表。与训练中一样，我们也假设在程序运行的开始和结束时都有null系统调用。异常现象可能有以下几种类型。

* 如果我们无法获取虚拟栈列表，则该栈一定是损坏的。这是一个堆栈异常。这种异常通常发生在粗缓冲区溢出攻击期间。
* 假设当前的虚拟栈列表是A = {a0, a1，…}。我们检查每一项是否在RA表中。如果有任何返回地址缺失，这是一个返回地址异常。
* 如果an没有正确的系统调用，这是一个系统调用异常。
* 我们根据公式(1)、(2)或(3)生成最后一次系统调用与当前系统调用之间的虚拟路径。如果该虚拟路径不在VP表中，则为虚拟路径异常。

FSA方法的一个问题是，入侵者可能会伪造一个溢出字符串，使调用栈看起来没有被破坏，但实际上却被破坏了，从而逃避检测。使用我们的方法，相同的攻击可能仍然会产生虚拟路径异常，因为调用堆栈被改变了。该方法在训练过程中使用并保存了更多的信息，使得攻击者更难逃避检测

开源之夏

开发记录

2023.6.25

* 首先根据进程指标（在线时间）找出异常进程，找到异常进程后拉起栈计数程序跟踪分析异常进程
* 直接使用栈计数程序跟踪分析所有进程的栈，找出数量异常的栈及相关的进程
* 应用在调试上一般直接跟踪相关进程
* 如果应用在系统异常检测上，应该每个进程分别检测
* 利用时序异常检测检测栈变化的异常，也分以上途径

![image](assets/image-20230714195231-0iad0zf.png)

![image](assets/image-20230714195302-xef2joa.png)​

![image](assets/image-20230714195408-1d0or4g.png)​​

2023.7.8

* 不同进程优先级不同，分得的时间片大小不同，定频采样所有进程的调用栈混在一起没有可比性
* 根据进程指标找出异常进程实际上也是混在一起比较，没有考虑优先级、控制组对资源的特定分配
* 应考虑每个进程分别检测
* 赵晨雨师兄建议将**内核栈和用户栈关联 ​**

2023.7.9

* 在跟踪所有进程时每个进程只能获取一个调用栈（暂时无法解释），所以跟踪所有进程时分别分析每个进程的主要调用栈的这种方式不可行
* 用来统计所有进程中特殊频次资源消耗的进程栈（目前实现）
* 在运行时设定要跟踪的特定的进程或者运行的命令（计划）
* 分析**特定进程的调用栈时序变化**

2023.7.10

* 想要跟踪的子进程或者线程可能在跟踪主进程时还未来得及创建，因此无法获取它们的pid
* attach_perf_event可以跟踪设定的pid的子进程
* 优于火焰图的地方，可以看出栈所属的进程
* on-cpu使用计数器而不是时间戳可以提高性能，减少记录次数

2023.7.11

* perf可以在无关栈顶函数的情况下记录栈
* 但off-cpu没办法计数，必须使用时间戳
* 如果要做hot-cold图的话on-cpu也必须以时间戳为单位
* 使用`sudo perf record -a -g -F 997 sleep 60`​、`perf script > perf_stack.log`​命令记录的stack信息如下：

  ```shell
  swapper     0 [003] 604164.215324:    1003009 cpu-clock:pppH: 
          ffffffff81f4108b native_safe_halt+0xb ([kernel.kallsyms])
          ffffffff81f4187b acpi_idle_enter+0xbb ([kernel.kallsyms])
          ffffffff81bb5697 cpuidle_enter_state+0x97 ([kernel.kallsyms])
          ffffffff81bb5cae cpuidle_enter+0x2e ([kernel.kallsyms])
          ffffffff81130493 call_cpuidle+0x23 ([kernel.kallsyms])
          ffffffff8113485d cpuidle_idle_call+0x11d ([kernel.kallsyms])
          ffffffff81134952 do_idle+0x82 ([kernel.kallsyms])
          ffffffff81134bbd cpu_startup_entry+0x1d ([kernel.kallsyms])
          ffffffff810880f2 start_secondary+0x122 ([kernel.kallsyms])
          ffffffff8100015a secondary_startup_64_no_verify+0xe5 ([kernel.kallsyms])
  ```

  头部的条目分别为：comm、tid、time、ip、sym，时间的格式是`s.ns`​

功能说明

> # stack_count
>
> 用于对调用栈进行计数，拥有对调用栈出现频次异常检测的能力。
>
> 启动后每5s输出一次从开始起记录到的进程栈及数量等信息。
>
> 在终止时将所有信息以json格式保存在`./stack_count.log`文件中。
>
> ## 输出片段
>
> 屏幕及文件输出：
>
> ```json
>   "273755":{
>     "name":"node",
>     "pids":{
>       "273755":{
>         "40":{
>           "trace":[
>             "0x7fab3b4dd97b:__clock_gettime_2",
>             "0x1548615:uv_run",
>             "0xa3fa35:_ZN4node13SpinEventLoopEPNS_11EnvironmentE",
>             "0xb42c56:_ZN4node16NodeMainInstance3RunEv",
>             "0xac89bc:_ZN4node5StartEiPPc",
>             "0x7fab3b423510:__libc_start_call_main"
>           ],
>           "count":1,
>           "label":null
>         }
>       }
>     }
>   },
> ```
>
> ## 相关选项
>
> - `u`: 跟踪用户空间调用栈而不是内核空间调用栈
> - `pid "pids"`: 设定需要跟踪的进程pid，可以是多个。不设定此选项会跟踪除idel外的所有进程
>
> ## 附加依赖
>
> python库：
>
> - psutil
> - pyod
>
> # load_monitor
>
> 用于在计算机负载超过阈值时记录内核栈数量信息，每5s输出一次总记录。
>
> 终止时将记录以 栈-数量 的格式保存在 `./stack.bpf` 中，并输出火焰图文件 `./stack.svg`
>
> ## 输出片段
>
> 屏幕输出：
>
> ```log
> ____________________________________________________________
> 0xffffffff928fced1 update_rq_clock
> 0xffffffff92904c34 do_task_dead
> 0xffffffff928c40a1 do_exit
> 0xffffffff928c421b do_group_exit
> 0xffffffff928d5280 get_signal
> 0xffffffff9283d6ce arch_do_signal_or_restart
> 0xffffffff9296bcc4 exit_to_user_mode_loop
> 0xffffffff9296be00 exit_to_user_mode_prepare
> 0xffffffff9359db97 syscall_exit_to_user_mode
> 0xffffffff93599809 do_syscall_64
> 0xffffffff93600099 entry_SYSCALL_64_after_hwframe
> stackid    count  pid    comm            
> 5          37    
>                   82731  5               
>                   82783  IPC I/O Parent  
>                   82794  TaskCon~ller #1 
>                   82804  pool-spawner    
>                   82830  Breakpad Server 
>                   82858  Socket Thread   
>                   82859  JS Watchdog     
>                   82860  Backgro~Pool #1 
>                   82861  Timer           
>                   82862  RemVidChild     
>                   82863  ImageIO         
>                   82864  Worker Launcher 
>                   82865  TaskCon~ller #0 
>                   82867  ImageBridgeChld 
>                   82869  ProfilerChild   
>                   82870  AudioIP~ack RPC 
>                   82871  AudioIP~ver RPC 
>                   82877  dconf worker    
>                   82885  snap            
>                   83010  StreamTrans #1  
>                   83011  StreamTrans #2  
>                   83018  StreamTrans #3  
>                   83020  StreamTrans #5  
>                   83029  JS Watchdog     
>                   83030  Backgro~Pool #1 
>                   83031  Timer           
>                   83033  ImageIO         
>                   83034  Worker Launcher 
>                   83036  TaskCon~ller #1 
>                   83037  ImageBridgeChld 
>                   83048  IPC I/O Child   
>                   83049  Socket Thread   
>                   83051  Backgro~Pool #1 
>                   83052  Timer           
>                   83053  RemVidChild     
>                   83055  TaskCon~ller #0 
>                   83059  ProfilerChild   
> ____________________________________________________________
> ```
>
> 文件输出：
>
> ```log
> @[
> update_rq_clock
> sched_autogroup_exit_task
> do_exit
> do_group_exit
> get_signal
> arch_do_signal_or_restart
> exit_to_user_mode_loop
> exit_to_user_mode_prepare
> syscall_exit_to_user_mode
> do_syscall_64
> entry_SYSCALL_64_after_hwframe
> ]: 37
> ```
>
> ## 附加依赖
>
> 程序：
>
> - `/usr/share/bcc/FlameGraph/stackcollapse-bpftrace.pl`
> - `/usr/share/bcc/FlameGraph/flamegraph.pl`

南帅波

**操作系统真相还原第六章（完善内核）**

**了解kvm内核虚拟化技术的基本原理**

**阅读论文《DBox：宏内核下各种设备驱动程序的高性能安全盒》**

---

‍

### KVM

* **在KVM中，一个虚拟机就是一个传统的Linux进程，拥有自己的PID，也可以用kill直接杀死(虚拟机内表现为突然断电), 一个Linux系统中，有多少个VM就有多少个进程**
* **KVM并不是一个完整的模拟器，而是一个提供了虚拟化功能的内核插件，具体具体的模拟工作借助于Qemu实现。(qemu是一个纯软件的虚拟化，性能较差)**

  * **KVM是一个内核模块，它直接集成在Linux内核中。KVM通过利用处理器的虚拟化扩展（如Intel VT（Virtualization Technology）或AMD-V（AMD Virtualization））提供硬件辅助的虚拟化功能。**
  * **QEMU则是一个用户态程序，它提供了虚拟机监视器（VMM）的功能，可以在用户态模拟各种硬件设备和处理器架构，以及提供对虚拟机的管理和控制。**

**虚拟化技术分类：**

* **全虚拟化：通过客户机和宿主机之间一个虚拟化逻辑层Hypervisor来完全模拟底层硬件细节**
* **半虚拟化： 是通过实现修改的客户机操作系统内核共享宿主机底层硬件来实现**

**虚拟化控制器（Hypervisor）**：底层物理设备与虚拟机之间的控制层，实现底层物理资源的抽象化和资源隔离，并对上层虚拟机运行进行控制。

* **Hypervisor的2种类型**：  
  **类型1（裸金属Hypervisor):运行在物理计算机的硬件上的一种Hypervisor。它直接控制物理计算机的资源，并且提供给虚拟机的硬件是经过虚拟化之后的，从而使得虚拟机可以直接访问物理硬件资源，性能比较好。（kvm）**  
  **类型2 （主机Hypervisor):Hypervisor运行在一个操作系统上，并且通过这个操作系统来管理和控制虚拟机。它需要利用操作系统来访问物理硬件资源，并且提供给虚拟机的硬件是经过模拟之后的，性能相对较差。(Oracle VirtualBox和VMware Workstation)**

![image-20230711204304358](https://gitee.com/nan-shuaibo/image/raw/master/202307120915920.png)​

![image-20230711205654173](https://gitee.com/nan-shuaibo/image/raw/master/202307120915871.png)​

![image-20230711205107056](https://gitee.com/nan-shuaibo/image/raw/master/202307120915523.png)​

**可以看出kvm的Makefile主要生成三个模块,kvm.o和kvm-intel.o、kvm-amd.o**

* **kvm.o是kvm的核心模块**

  * **IOMMU、中断控制、设备管理、kvm arch等部分代码**
  * **kvm并没有完全实现一整个PC系统虚拟化，而仅仅将部分重要的CPU虚拟化、I/O虚拟化和内存虚拟化部分针对硬件辅助的能力进行有效地抽象和对接，其他一些模块需要借助于Qemu**
  * **也就是说，kvm基本只实现硬件辅助虚拟化相关部分，而不支持的用Qemu来模拟实现**
* **kvm-intel.o是intel平台架构虚拟化模块，平台相关**
* **kvm-amd.o是amd架构虚拟化模块，平台相关**

**guest OS保证具体运行场景中的程序正常执行，而KVM的代码则部署在HOST上，Userspace对应的是QEMU，Kernel对应的是KVM Driver，KVM Driver负责模拟虚拟机的CPU运行，内存管理，设备管理等；QEMU则模拟虚拟机的IO设备接口以及用户态控制接口。QEMU通过KVM等fd进行IOCTL控制KVM驱动的运行过程。**

**kvm模型结构：**

![1534126853439.png](https://gitee.com/nan-shuaibo/image/raw/master/202307120915579.png)​

**如上图所示，guest自身有自己的用户模式和内核模式；**

* **guest是在host中是作为一个用户态进程存在的，这个进程就是qemu，qemu本身就是一个虚拟化程序，只是纯软件虚拟化效率很低，它被KVM进行改造后，作为KVM的前端存在，用来进行创建进程或者IO交互等；**
* **KVM和Qemu的结合是开源社区相互协作的典例。**
* **KVM Driver则是Linux内核模式，它提供KVM fd给qemu调用，用来进行cpu虚拟化，内存虚拟化等。**
* **Qemu 通过 /dev/kvm 接口设置一个虚拟机的地址空间，然后向它提供模拟好的 I/O 设备，并将相关的设备回显操作映射到宿主机，完成整个 I/O 设备的虚拟化操作。**
* **/dev/kvm 接口是 Qemu 和 KVM 交互的“桥梁”，基本的原理是：/dev/kvm 本身是一个设备文件，这就意味着可以通过 ioctl 函数来对该文件进行控制和管理，从而可以完成用户空间与内核空间的数据交互。kvm负责接收qemu模拟效率很低的命令,在 KVM 与 Qemu 的通信过程主要就是一系列针对该设备文件的 ioctl 调用。**

**kvm工作原理：**

![1534127005222.png](https://gitee.com/nan-shuaibo/image/raw/master/202307120915631.png)​

**上图是一个执行过程图，**

* **首先启动一个虚拟化管理软件qemu，开始启动一个虚拟机**
* **通过ioctl等系统调用向内核中申请指定的资源，搭建好虚拟环境，启动虚拟机内的OS，执行 VMLAUCH 指令，即进入了guest代码执行过程。**
* **如果 Guest OS 发生外部中断或者影子页表缺页之类的事件，暂停 Guest OS 的执行，退出QEMU即guest VM-exit，进行一些必要的处理，然后重新进入客户模式，执行guest代码；这个时候如果是io请求，则提交给用户态下的qemu处理，qemu模拟处理后再次通过IOCTL反馈给KVM驱动。**

**hypercall超级调用：当虚拟机的Guest OS需要执行一些更高权限的操作（如：页表的更新、对物理资源的访问等）时，由于自身在非特权域无法完成这些操作，于是便通过调用Hypercall交给Hypervisor来完成这些操作。**

**kvm相关的tracepoint（/arch/x86/kvm/trace.h）**

**kvm_entry和kvm_exit被触发的几种情况，启动虚拟机，kvm中发生中断，kvm中进程主动让出CPU等**

![image-20230711222813871](https://gitee.com/nan-shuaibo/image/raw/master/202307120915983.png)​

![image-20230711222839625](https://gitee.com/nan-shuaibo/image/raw/master/202307120915684.png)​

![image-20230711222911490](https://gitee.com/nan-shuaibo/image/raw/master/202307120915466.png)​

**ebpf程序监控kvm事件：**

![image-20230711223925786](https://gitee.com/nan-shuaibo/image/raw/master/202307120915351.png)​

![image-20230711223820201](https://gitee.com/nan-shuaibo/image/raw/master/202307120915123.png)​

**exit_reason：**

**12，18，32分别表示访问任务优先级寄存器（TP），操作系统特定事件，处理器处于AP复位保持状态**

![image-20230711225439205](https://gitee.com/nan-shuaibo/image/raw/master/202307120915208.png)​

**hypercall nr：**

**5号超级调用表示请求Hypervisor程序将处于休眠状态的 vCPU 唤醒**

![image-20230711230136383](https://gitee.com/nan-shuaibo/image/raw/master/202307120916480.png)​

### DBox：宏内核下各种设备驱动程序的高性能安全盒

**在用户空间运行部分驱动程序，此类方法无法将内核与易受攻击的驱动程序完全隔离，并引入上下文切换的额外性能开销 . 通过页表限制驱动程序访问权限，在内核API或驱动程序接口函数被调用时产生的页表切换，内核分配或释放动态数据块时产生的页表权限修改与检查动作，以及监视内核 API调用情况的动作频繁出现时，会造成明显的性能开销 .**

**DBox，它是针对设备驱动程序基于虚拟化技术和 x86 架构特性的面向性能的“盒子”框架. 它在主机OS中提供了一个基于VM的驱动程序盒，用于隔离驱动程序，称为设备驱动虚拟机DDVM（Device Driver Virtual Machine），并提供了相应的DDVM管理组件和高效的I/O接口，命名为通用通信接口驱动 CCID（Common CommunicationInterface Driver）。DBox是在不牺牲I/O性能的情况下全可靠地隔离宏内核中原始设备驱动程序的轻量级框架，由 DDVM 和 CCID 两 个组件组成 .DDVM是一个通过KVM创建的安全容器，用于“装箱”宏内核的原始驱动程序. CCID是一个主机内核模块，用于管理DDVM，并在主机内核和DDVM之间提供高度优化的通信机制 .**

‍
