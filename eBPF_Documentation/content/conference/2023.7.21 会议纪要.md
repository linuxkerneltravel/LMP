### CPU组：

成员：杨宁珂、张子恒、王越

**杨宁珂**

本周工作：

- 输出libbpf代码，可以查看进程的一部分数据，目前还剩下进程间通信方式，进程中包含线程数量，这
  些数据还没产出（主要是查看相关内核函数还是比较麻烦）
- XGboost算法也已经有了基本雏形，就等根据上述信息产出之后进行输入分析
- 内核当中函数的一个个分析，主要在自己做的过程当中，发现调用函数，不知道这些内核函数都代表
  了什么意义，所以自己整理了一下

下周任务：

- 将libbpf代码产出完成
- 进程实验结尾
- 将自己手头的东西进行整理，然后写成文档，着手准备中国软件开源创新大赛

#### libbpf 程序代码以及结果

以下都是内核态的主要代码

```c
SEC("tp/sched/sched_process_exec")
int handle_exec(struct trace_event_raw_sched_process_exec *ctx)
{
struct task_struct *task;
unsigned fname_off;
struct event *e;
pid_t pid;
u64 ts;
/* remember time exec() was executed for this PID */
pid = bpf_get_current_pid_tgid() >> 32;
ts = bpf_ktime_get_ns();
bpf_map_update_elem(&exec_start, &pid, &ts, BPF_ANY);
/* don't emit exec events when minimum duration is specified */
if (min_duration_ns)
return 0;
/* reserve sample from BPF ringbuf */
e = bpf_ringbuf_reserve(&rb, sizeof(*e), 0);
if (!e)
return 0;
/* fill out the sample with data */
task = (struct task_struct *)bpf_get_current_task();
e->exit_event = false;
e->pid = pid;
e->ppid = BPF_CORE_READ(task, real_parent, tgid);
bpf_get_current_comm(&e->comm, sizeof(e->comm));
fname_off = ctx->__data_loc_filename & 0xFFFF;
bpf_probe_read_str(&e->filename, sizeof(e->filename), (void *)ctx +
fname_off);
/* successfully submit it to user-space for post-processing */
bpf_ringbuf_submit(e, 0);
return 0;
}
```

```c
SEC("tp/sched/sched_process_exit")
int handle_exit(struct trace_event_raw_sched_process_template *ctx)
{
struct task_struct *task;
struct event *e;
pid_t pid, tid;
u64 id, ts, *start_ts, duration_ns = 0;
/* get PID and TID of exiting thread/process */
id = bpf_get_current_pid_tgid();
pid = id >> 32;
tid = (u32)id;
/* ignore thread exits */
if (pid != tid)
return 0;
/* if we recorded start of the process, calculate lifetime duration */
start_ts = bpf_map_lookup_elem(&exec_start, &pid);
if (start_ts)
duration_ns = bpf_ktime_get_ns() - *start_ts;
else if (min_duration_ns)
return 0;
bpf_map_delete_elem(&exec_start, &pid);
/* if process didn't live long enough, return early */
if (min_duration_ns && duration_ns < min_duration_ns)
return 0;
/* reserve sample from BPF ringbuf */
e = bpf_ringbuf_reserve(&rb, sizeof(*e), 0);
if (!e)
return 0;
/* fill out the sample with data */
task = (struct task_struct *)bpf_get_current_task();
e->exit_event = true;
e->duration_ns = duration_ns;
e->pid = pid;
e->ppid = BPF_CORE_READ(task, real_parent, tgid);
e->exit_code = (BPF_CORE_READ(task, exit_code) >> 8) & 0xff;
bpf_get_current_comm(&e->comm, sizeof(e->comm));
/* send data to user-space for post-processing */
改动libbpf文件得到进程的CPU占用率
bpf_ringbuf_submit(e, 0);
return 0;
}
```

#### 改动libbpf文件得到进程的CPU占用率

#### 内存占用量

因为无法使用libbpf得到进程的内存占用量，所以我使用 /proc/<pid>/statm 文件计算进程内存使用量

```c
#include <stdio.h>
#include <stdlib.h>
int get_memory_usage(int pid) {
  char filename[256];
  sprintf(filename, "/proc/%d/statm", pid);
  FILE* fp = fopen(filename, "r");
  if (!fp) {
    fprintf(stderr, "failed to open %s\n", filename);
    return -1;
 }
  int pagesize = getpagesize();
  int rss_pages, vsz_pages;
  if (fscanf(fp, "%d %d", &rss_pages, &vsz_pages) != 2) {
    fprintf(stderr, "failed to read %s\n", filename);
    fclose(fp);
    return -1;
 }
  fclose(fp);
  int rss_bytes = rss_pages * pagesize;
  int vsz_bytes = vsz_pages * pagesize;
  printf("Process %d memory usage: %d KB (RSS), %d KB (VSZ)\n", pid, rss_bytes
/ 1024, vsz_bytes / 1024);
  return 0;
}
int main(int argc, char **argv) {
  if (argc != 2) {
    fprintf(stderr, "Usage: %s <pid>\n", argv[0]);
    return 1;
 }
  int pid = atoi(argv[1]);
  if (pid <= 0) {
    fprintf(stderr, "Invalid PID: %s\n", argv[1]);
    return 1;
 }
  get_memory_usage(pid);
  return 0;
}
```

得到进程的内存占用量，其中RSS是物理内存的占用量包括了进程的代码、数据和共享库等部分，但不
包括交换空间（swap）中的部分。，VSZ是当前虚拟内存大小包括了RSS、交换空间以及未分配的内存
等部分。

#### 进程优先级

```c
sudo perf record -e sched:sched_switch -a -g -- sleep 10
sudo perf script > output.txt
```

这是一条Linux内核事件跟踪（Event Tracing for Linux，简称ETL）记录，它记录了一个进程切换事
件。下面是这条记录的每个字段的含义：

- migration/0 : 前一个进程的名称，表示进程的调度已经完成，进入了就绪队列（Ready
  Queue），等待下一次调度。
- 15 [000] : 前一个进程的进程ID和CPU编号，表示该进程运行在CPU 0 上。
- 18090.199920 : 时间戳，表示事件发生的时间，单位为秒。
- sched_switch : 事件名称，表示进程切换事件。
- prev_comm=migration/0 : 前一个进程的名称，与上面的相同。
- prev_pid=15 : 前一个进程的进程ID，与上面的相同。
- prev_prio=0 : 前一个进程的静态优先级，为0表示是一个空闲进程。
- prev_state=S : 前一个进程的状态，S表示进程状态为休眠（Sleeping）。
- next_comm=swapper/0 : 下一个进程的名称，表示下一个进程是内核线程swapper/0。
- next_pid=0 : 下一个进程的进程ID，为0表示是一个空闲进程。
- next_prio=120 : 下一个进程的静态优先级，为120表示是一个高优先级进程。

接下来的每一行是进程切换事件的调用栈，记录了事件发生时内核的执行情况。可以看到，进程切换事
件是由 __schedule() 函数触发的，它被调用了两次，并最终调用了 smpboot_thread_fn() 函数和
kthread() 函数，表示正在执行内核线程swapper/0。

这条记录的含义是，一个进程（migration/0）已经被调度出CPU 0，然后内核调度器（scheduler）将
CPU 0 分配给了内核线程swapper/0。记录中的调用栈信息可以帮助开发人员分析系统的性能瓶颈，并
进行优化.

#### XGboost算法

数据集包含以下特征：A、B、C、D和E

```c
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
# 假设你已经加载了数据并将其分为特征和目标变量
X = bank_data.drop('default', axis=1)
y = bank_data['default']
# 划分训练集和测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
random_state=42)
# 将数据转换为DMatrix格式
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)
# 设置参数
params = {
  'objective': 'binary:logistic',  # 二分类问题使用逻辑回归损失函数
  'eta': 0.1,  # 学习率
  'max_depth': 3,  # 树的最大深度
  'subsample': 0.8,  # 每棵树样本的采样比例
  'colsample_bytree': 0.8  # 每棵树特征的采样例
}
# 训练模型
num_rounds = 100  # 迭代次数
model = xgb.train(params, dtrain, num_rounds)
# 在测试集上进行预测
y_pred = model.predict(dtest)
y_pred_binary = [1 p >= 0.5 else 0 for p in y_pred]  # 将概率转换为二分类标签
# 计算准确率和混淆矩阵
accuracy = accuracy_score(y_test, y_pred_binary)
confusion_mat = confusion_matrix(y_test, y_pred_binary)
print("Accuracy:", accuracy)
print("Confusion Matrix:")
print(confusion_mat)
```

此代码就是已经加载数据集，并将其分特征（X）和目标量（y）

#### 根据自己的需要查找的内核函数

**static_prio**
进程的静态优先级，静态优先级不会随着时间改变，但是可以通过nice或sched_setscheduler系统调用
修改。
**normal_prio**
基于static_prio和调度策略计算出来的优先级，do_fork进程时，子进程会继承父进程的normal_prio。
对于普通进程，normal_prio等于static_prio；对于实时进程，会根据rt_priority计算normal_prio。
**prio**
进程的动态优先级。
**rt_priority**
实时进程优先级

#### perf_event

**cpu维度**

使用perf_event_context类型的链表来连接本cpu的相关perf_event。这样的链表共有两条
(perf_hw_context = 0, perf_sw_context = 1)，链表指针存放在per_cpu

**task维度**

使用perf_event_context类型的链表来连接本task的相关perf_event。这样的链表共有两条
(perf_hw_context = 0, perf_sw_context = 1)，链表指针存放在task->perf_event_ctxp[ctxn]变量中

perf_event_open()系统调用使用cpu、pid两个参数来指定perf_event的cpu、task维度
pid == 0: event绑定到当前进程；
pid > 0: event绑定到指定进程；
pid == -1: event绑定到当前cpu的所有进程。
cpu >= 0: event绑定到指定cpu；
cpu == -1: event绑定到所有cpu；

**张子恒**
本周工作：

1. 开源之夏，通过分析现有程序的挂载点以及内核源码，已确定程序的设计，目前数据提取方面就差
参数信息
2. 阅读真相还原
下周工作：
1. 结合源码，对开源之夏的工具进行编写
2. 有条不紊的推进真相还原的阅读
3. 阅读一篇论文

### 文件系统组：

成员：廉洋洋、张帆

**廉洋洋**

上汽项目的当前进展：

第一阶段：已经提出了文件系统微内核的方案：FUSE

第二阶段：对于FUSE文件系统进行安全及性能方面的功能提升

​		1）安全方面：FUSE+ACL—>FUSE+SandFS

​		2）性能方面：FUSE read性能提升（可以与小论文相结合）

#### 1、FUSE+ACL目前的进展

**实现方法**：

1、用 ACL 库作为 ACL 信息的存放地

​		构造一个 ACL 库，将 所有的 ACL 信息均存放到这个库中。在进行所有 的文件操作之前都根据文件名在 ACL 库中进行查找，得到该文件的 ACL 信息，再调用权限检查函数，检查当前用户是否有权限进行相应的文件操作。如 果能够进行相应的操作，则调用低层文件系统的相 应功能来完成指定的文件操作。它所有的操作都是 在 VFS 层进行的。

优点：由于在 VFS 层进行存储控制，故不 管低层的文件系统是什么，都可以实现 ACL 功能，因此具有较强的通用性。

缺点：由于将所有 文件的 ACL 信息都存放在一个 ACL 库中，因此它对维护和性能方面有很大影响

2、将文件的 ACL 信息存放到具体的文件中

​		将某一个文件的 ACL 信息存储在该文件中，在读该文件的 Inode 的时候将 该 ACL 信息读出，在写该文件的 Inode 的时候将 ACL 信息写回。对相应的低层文件系统的 permission 函 数进行改进使其根据从文件中读出的 ACL 信息和 当前用户的用户 ID、组 ID 及当前用户提出的操作类 型作出能否进行操作的决定。它所有的操作都是在 低层的文件系统中进行的。

优点：由于将文件的 ACL 信息分散地存 储到相应的文件中，在对文件操作进行相应的权限检查时，面对的 ACL 信息非常有限，因此文件系统 性能的下降并不是很明显。

缺点：由于是在低层的文件系统进行的。因此对应不同的文件系统都要进行不同的代码改动，代码的改动量比较大。

**此处FUSE+ACL的实现策略选择了第二种方法，将ACL信息作为文件的扩展属性进行操作。**

目前，改动了一部分代码，实际功能还未真正实现。

#### 2、FUSE read性能提升（可以与小论文相结合）

1、开源之夏大赛中对于FUSE read操作中元数据的优化

2、结合小论文的创新点：predictive prefetching can use AI algorithms to adapt to a specificuser’s workload(预测性预取，可以使用AI算法来适应特定用户的工作负载)

​		**预测性预取**，利用AI算法来适应特定用户的工作负载。预测性预取是一种通过提前获取数据或资源，以减少延迟并提高性能的技术。它**基于对用户行为和模式的分析**，**预测用户接下来可能需要的数据**，**并在需要时提前将这些数据加载到内存或缓存中**。

​		通过使用AI算法进行预测性预取，系统可以根据特定用户的工作负载进行智能优化，提前预取所需的数据，以**提高性能和用户体验**。

目前，实现了数据采集的libbpf程序的初步编写，调研了四种基于FUSE的用户态分布式文件系统： Ceph、EOS、GlusterFS、Lustre。下一步打算选择其中一种文件系统进行部署，利用libbpf程序采集用户行为的相关数据进行分析预测。

### 鸿蒙组：

成员：刘冰、南帅波

**刘冰**

- 开源之夏
- 撰写鸿蒙总结报告（kal层和分布式软总线部分）

**南帅波**

- 真相还原第六章
- 撰写鸿蒙研究总结报告（hdf驱动框架和开发板平台适配部分）